{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNi0q8e9Ys3H9XfKhLzKShe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KunalRoutray/Deep-Learning-With-Tensorflow/blob/main/ASSIGNMENT_4(DLWTF_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "kunal routray\n",
        "\n",
        "2341018202\n",
        "\n",
        "**WORD EMBEDDING**"
      ],
      "metadata": {
        "id": "bAE6shJq5jh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Import text8 dataset of word2vec from gensim model and perform the following task.**"
      ],
      "metadata": {
        "id": "hPJ7wabrunq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n"
      ],
      "metadata": {
        "id": "Rg-16KXOu-_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09fcd562-f7a4-4186-87f5-7bddf16647c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1(a) Import the word vector model**"
      ],
      "metadata": {
        "id": "l-HrMbXMu96_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "import os\n",
        "info = api.info(\"text8\")\n",
        "assert(len(info) > 0)\n",
        "dataset = api.load(\"text8\")\n",
        "model = Word2Vec(dataset)\n",
        "# Create the data directory if it doesn't exist\n",
        "if not os.path.exists(\"data\"):\n",
        "   os.makedirs(\"data\")\n",
        "model.save(\"data/text8-word2vec.bin\")\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "def print_most_similar(word_conf_pairs, k):\n",
        "   for i, (word, conf) in enumerate(word_conf_pairs):\n",
        "       print(\"{:.3f} {:s}\".format(conf, word))\n",
        "       if i >= k-1:\n",
        "          break\n",
        "       if k < len(word_conf_pairs):\n",
        "          print(\"...\")\n",
        "model = KeyedVectors.load(\"data/text8-word2vec.bin\")\n",
        "word_vectors = model.wv\n",
        "print(word_vectors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aybN7Pi5vE8",
        "outputId": "006e7590-e1e9-4544-937d-026d7265c4fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n",
            "KeyedVectors<vector_size=100, 71290 keys>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1(b) Write Python code to print the vocabulary to show first 15 words.**"
      ],
      "metadata": {
        "id": "1dDEaBtR5zOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_vectors.key_to_index.keys() #stores all words seen during tra\n",
        "print([x for i, x in enumerate(words) if i < 15])\n",
        "assert(\"king\" in words)\n",
        "print(\"words similar to king\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp0E5Szd5z1Q",
        "outputId": "763ce58e-4f49-4bda-f929-928cefd35be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'two', 'is', 'as', 'eight', 'for', 's']\n",
            "words similar to king\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1(c) Write Python code to print the most similar words to “king”.**\n"
      ],
      "metadata": {
        "id": "tL-sjIyo520n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_most_similar(word_vectors.most_similar(\"king\"), 5) #returns list of t\n",
        "print(\" vector arithmetic with words (cosine similarity)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP8hyOSv55VE",
        "outputId": "b911f60e-44b8-4baa-ae6c-bc22bfaf516e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.710 prince\n",
            "...\n",
            "0.707 queen\n",
            "...\n",
            "0.705 emperor\n",
            "...\n",
            "0.701 kings\n",
            "...\n",
            "0.696 throne\n",
            " vector arithmetic with words (cosine similarity)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1(d) Write Python code to print the most similar words to “king”.\n",
        "d Write a Python code to perform an analogy test: France is to Paris? is to Berlin . Analogy\n",
        "a− b + c\n",
        "here, positive[a,c] , negative[b], which is equivalent to a is to b as c is to .**\n"
      ],
      "metadata": {
        "id": "M1FLNX9I5-to"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\" france + berlin - paris = ?\")\n",
        "print_most_similar(word_vectors.most_similar(positive=[\"france\", \"berlin\"], negative=[\"paris\"]), 1)\n",
        "print(\"vector arithmetic with words (Levy and Goldberg)\")\n",
        "print(\"france + berlin - paris = ?\")\n",
        "print_most_similar(word_vectors.most_similar_cosmul(\n",
        "positive=[\"france\", \"berlin\"], negative=[\"paris\"]), 1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LhM2lDD6A3X",
        "outputId": "41f8e596-56e3-4b1c-b5be-bc6cec8543b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " france + berlin - paris = ?\n",
            "0.799 germany\n",
            "vector arithmetic with words (Levy and Goldberg)\n",
            "france + berlin - paris = ?\n",
            "0.976 germany\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1(e) Write a Python code to find odd one out from the list [”hindus”, ”parsis”, ”singapore”, ”christians”]**"
      ],
      "metadata": {
        "id": "FP8BQXMR6Fo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"find odd one out\")\n",
        "print(\"[hindus, parsis, singapore, christians]\")\n",
        "print(word_vectors.doesnt_match([\"hindus\", \"parsis\", \"singapore\", \"christian\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdi37wU_6FGZ",
        "outputId": "9dbd36dc-79bf-44fd-f15b-067801399040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find odd one out\n",
            "[hindus, parsis, singapore, christians]\n",
            "singapore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1(f) Write a Python code to find the similarity of man with woman, dog,whale and tree**"
      ],
      "metadata": {
        "id": "tk838Z7m6Jnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"similarity between words\")\n",
        "for word in [\"woman\", \"dog\", \"whale\", \"tree\"]:\n",
        "    print(\"similarity({:s}, {:s}) = {:.3f}\".format( \"man\", word, word_vectors.similarity(\"man\", word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgKcvap36KUO",
        "outputId": "940e95eb-0e11-4984-aab6-438ee197051c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity between words\n",
            "similarity(man, woman) = 0.735\n",
            "similarity(man, dog) = 0.451\n",
            "similarity(man, whale) = 0.273\n",
            "similarity(man, tree) = 0.257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1(g) Write a Python code to find similar by word to Singapore.**"
      ],
      "metadata": {
        "id": "5U97EptP6PpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"similar by word\")\n",
        "print_most_similar(word_vectors.similar_by_word(\"singapore\"), 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndOa5w6n6Qk0",
        "outputId": "c21d66e9-c63c-4c34-9e17-209a2d653538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similar by word\n",
            "0.898 malaysia\n",
            "...\n",
            "0.842 uganda\n",
            "...\n",
            "0.836 philippines\n",
            "...\n",
            "0.833 indonesia\n",
            "...\n",
            "0.826 thailand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1(h) Write a Python code to find the distance between two vectors like Singapore and ‘Malaysia**"
      ],
      "metadata": {
        "id": "8ayWjy0X6VJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"distance between vectors\")\n",
        "print(\"distance(singapore, malaysia) = {:.3f}\".format(word_vectors.distance(\"singapore\", \"malaysia\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxCSLZz-6WGU",
        "outputId": "4dc8518d-4698-4253-a4b0-280587717896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distance between vectors\n",
            "distance(singapore, malaysia) = 0.102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Write Python code to find the most similar word for ‘machine’ from the given sentence.The Sentence is  “I love machine learning. Machine Learning is fun.”**"
      ],
      "metadata": {
        "id": "kJaQIjSK6Zq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [[\"i\",\"love\",\"machine\",\"learning\"],[\"machine\",\"learning\",\"is\",\"fun\"]]\n",
        "\n",
        "# CBOW: sg=0 (skip-gram would be sg=1)\n",
        "model = Word2Vec(\n",
        "sentences,\n",
        "vector_size=100,\n",
        "# embedding dim\n",
        "window=2,\n",
        "min_count=1,\n",
        "workers=4,\n",
        "sg=0,\n",
        "negative=10,\n",
        "epochs=30\n",
        ")\n",
        "print(\"Vector for 'learning':\", model.wv[\"learning\"][:5])\n",
        "print(\"Most similar to 'machine':\", model.wv.most_similar(\"machine\", topn=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxs1ARw06bph",
        "outputId": "aab52d46-a597-4e7b-b03e-02c8b7081f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'learning': [-0.00053623  0.00023643  0.00510335  0.00900927 -0.00930295]\n",
            "Most similar to 'machine': [('is', 0.06797364354133606), ('i', 0.009403552860021591), ('love', 0.004535813815891743), ('learning', -0.010849320329725742), ('fun', -0.023415347561240196)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. Write Python code to implement CBoW architecture from scratch.For the setence “I love machine learning.\" find the nearest word for learning,machine and love. Use Word2Vec and CBoW to find the most similar words.**"
      ],
      "metadata": {
        "id": "KWwgM2946fhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "raw_sentences = [\"I love machine learning\"]\n",
        "def normalize(s):\n",
        "   s = s.lower()\n",
        "   s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
        "   s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "   return s\n",
        "sentences = [normalize(s).split() for s in raw_sentences]\n",
        "# 1) Build vocab\n",
        "from collections import Counter\n",
        "word_counts = Counter([w for sent in sentences for w in sent])\n",
        "itos = sorted(word_counts.keys())\n",
        "stoi = {w:i for i,w in enumerate(itos)}\n",
        "V = len(stoi)\n",
        "# 2) Build CBOW training pairs\n",
        "# context window = W (left W, right W)\n",
        "W = 2 # window size\n",
        "contexts, targets = [], []\n",
        "for sent in sentences:\n",
        "    idxs = [stoi[w] for w in sent]\n",
        "    for i, center in enumerate(idxs):\n",
        "        ctx = []\n",
        "        for k in range(1, W+1):\n",
        "            if i-k >= 0: ctx.append(idxs[i-k])\n",
        "            if i+k < len(idxs): ctx.append(idxs[i+k])\n",
        "        if len(ctx) > 0: # allow edges (variable ctx length)\n",
        "           # pad or crop to fixed 2*W so model input is fixed\n",
        "           # simple strategy: if short, repeat ends\n",
        "           while len(ctx) < 2*W:\n",
        "                ctx.append(ctx[-1])\n",
        "           if len(ctx) > 2*W:\n",
        "              ctx = ctx[:2*W]\n",
        "           contexts.append(ctx)\n",
        "           targets.append(center)\n",
        "contexts = np.array(contexts, dtype=np.int32) # shape (N, 2W)\n",
        "targets = np.array(targets, dtype=np.int32) # shape (N,)\n",
        "y_onehot = tf.keras.utils.to_categorical(targets, num_classes=V)\n",
        "print(f\"Vocab size: {V}, pairs: {len(targets)}, window: {W}\")\n",
        "print(\"Example context idxs:\", contexts[0], \"-> target idx:\", targets[0])\n",
        "\n",
        "# 3) Define CBOW model\n",
        "class CBOWModel(tf.keras.Model):\n",
        "      def __init__(self, vocab_sz, emb_sz, window_sz, **kwargs):\n",
        "          super().__init__(**kwargs)\n",
        "          self.window_sz = window_sz\n",
        "          self.embedding = layers.Embedding(\n",
        "               input_dim=vocab_sz,\n",
        "               output_dim=emb_sz,\n",
        "               embeddings_initializer=\"glorot_uniform\",\n",
        "               mask_zero=False, # we padded by repetition, not zer\n",
        "               name=\"embedding\"\n",
        "          )\n",
        "          self.dense = layers.Dense(\n",
        "             vocab_sz,\n",
        "             kernel_initializer=\"glorot_uniform\",\n",
        "             activation=\"softmax\",\n",
        "             name=\"softmax_head\"\n",
        "           )\n",
        "      def call(self, x):\n",
        "          x = self.embedding(x)\n",
        "          x = tf.reduce_mean(x, axis=1) # (batch, emb)\n",
        "          x = self.dense(x) # (batch, V)\n",
        "          return x\n",
        "VOCAB_SIZE = V\n",
        "EMBED_SIZE = 100\n",
        "WINDOW_SIZE = W\n",
        "model = CBOWModel(VOCAB_SIZE, EMBED_SIZE, WINDOW_SIZE)\n",
        "model.build(input_shape=(None, WINDOW_SIZE * 2))\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "loss=\"categorical_crossentropy\",\n",
        "metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# 4) Train\n",
        "history = model.fit(contexts, y_onehot, batch_size=32,\n",
        "epochs=100,\n",
        "verbose=0,\n",
        "validation_split=0.2\n",
        ")\n",
        "print(f\"Final train acc: {history.history['accuracy'][-1]:.3f} | \"\n",
        "f\"val acc: {history.history['val_accuracy'][-1]:.3f}\")\n",
        "\n",
        "# 5) Extract embeddings\n",
        "emb_weights = model.get_layer(\"embedding\").get_weights()[0] # (V, EMBED_SI\n",
        "print(\"Embedding matrix shape:\", emb_weights.shape)\n",
        "\n",
        "# 6) Utility: most similar words (cosine)\n",
        "def most_similar(query, topk=5):\n",
        "    if query not in stoi:\n",
        "       return []\n",
        "    qv = emb_weights[stoi[query]][None, :]\n",
        "    sims = cosine_similarity(qv, emb_weights)[0]\n",
        "    order = np.argsort(-sims)\n",
        "    out = []\n",
        "    for idx in order:\n",
        "        w = itos[idx]\n",
        "        if w == query:\n",
        "           continue\n",
        "        out.append((w, float(sims[idx])))\n",
        "        if len(out) >= topk:\n",
        "           break\n",
        "    return out\n",
        "print(\"\\nNearest to 'learning':\", most_similar(\"learning\"))\n",
        "print(\"Nearest to 'machine':\", most_similar(\"machine\"))\n",
        "print(\"Nearest to 'love':\", most_similar(\"love\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "C_y82D3G6jhc",
        "outputId": "a4be3494-1cce-4258-8053-aa94d1d514e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 4, pairs: 4, window: 2\n",
            "Example context idxs: [2 3 3 3] -> target idx: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'cbow_model', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"cbow_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cbow_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ softmax_head (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ softmax_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final train acc: 1.000 | val acc: 0.000\n",
            "Embedding matrix shape: (4, 100)\n",
            "\n",
            "Nearest to 'learning': [('i', 0.3010336756706238), ('love', -0.05245177447795868), ('machine', -0.07170701026916504)]\n",
            "Nearest to 'machine': [('love', 0.01448216661810875), ('learning', -0.07170701026916504), ('i', -0.2994571328163147)]\n",
            "Nearest to 'love': [('i', 0.021397151052951813), ('machine', 0.01448216661810875), ('learning', -0.05245177447795868)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. Download GloVe embedding and find the most similar word to king using cosine similarity.**\n"
      ],
      "metadata": {
        "id": "MVprTOGP6kYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download GloVe embedding\n",
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMNllEuE6n-K",
        "outputId": "af4bc282-30cf-4fed-a05c-c798cc87856d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-10 09:44:26--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2026-02-10 09:44:27--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.89MB/s    in 2m 43s  \n",
            "\n",
            "2026-02-10 09:47:11 (5.03 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find the most similar word to king using cosine similarity.\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "GLOVE_PATH = \"/content/glove.6B.100d.txt\" # change path if needed\n",
        "word2vec = {}\n",
        "with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
        "  for line in tqdm(f, total=400000): # 400k vocab in glove.6B\n",
        "      parts = line.strip().split()\n",
        "      word = parts[0]\n",
        "      vec = np.array(parts[1:], dtype=np.float32)\n",
        "      word2vec[word] = vec\n",
        "print(f\"Loaded {len(word2vec)} words. Vector size = {len(next(iter(word2vec.values())))}\")\n",
        "\n",
        "# 2. Define function to find most similar word\n",
        "def most_similar(word, topk=5):\n",
        "  if word not in word2vec:\n",
        "    print(f\"'{word}' not in vocabulary.\")\n",
        "    return []\n",
        "  q_vec = word2vec[word].reshape(1, -1)\n",
        "  all_words = list(word2vec.keys())\n",
        "  all_vectors = np.array(list(word2vec.values()))\n",
        "  # cosine similarity\n",
        "  sims = cosine_similarity(q_vec, all_vectors)[0]\n",
        "  # sort descending (excluding the word itself)\n",
        "  top_idx = np.argsort(-sims)[1:topk+1]\n",
        "  return [(all_words[i], float(sims[i])) for i in top_idx]\n",
        "\n",
        "print(\"\\nMost similar to 'king':\")\n",
        "print(most_similar(\"king\", topk=10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "283AaEgq6qBJ",
        "outputId": "0815fd98-e4d2-4fa1-dc68-c58a3314aedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400000/400000 [00:09<00:00, 43284.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 words. Vector size = 100\n",
            "\n",
            "Most similar to 'king':\n",
            "[('prince', 0.7682329416275024), ('queen', 0.7507690191268921), ('son', 0.7020888924598694), ('brother', 0.6985775232315063), ('monarch', 0.6977890729904175), ('throne', 0.6919990181922913), ('kingdom', 0.6811410188674927), ('father', 0.6802029609680176), ('emperor', 0.6712857484817505), ('ii', 0.6676074266433716)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. Write a Python code to compute the similarity between “man” and other words\n",
        "(“woman”, “dog”, “whale”, “tree”) using GloVe embeddings**"
      ],
      "metadata": {
        "id": "ezbyUUEh6tdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(word1, word2):\n",
        "  if word1 not in word2vec or word2 not in word2vec:\n",
        "    return None\n",
        "  v1 = word2vec[word1].reshape(1, -1)\n",
        "  v2 = word2vec[word2].reshape(1, -1)\n",
        "  return float(cosine_similarity(v1, v2)[0][0])\n",
        "target = \"man\"\n",
        "others = [\"woman\", \"dog\", \"whale\", \"tree\"]\n",
        "print(f\"\\nCosine similarity with '{target}':\\n\")\n",
        "for w in others:\n",
        "    sim = similarity(target, w)\n",
        "    print(f\"{target:>5s} - {w:<6s} = {sim:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8cYVNcc6vLZ",
        "outputId": "5205dd67-aaa6-4672-dcd4-e251aedcaf87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cosine similarity with 'man':\n",
            "\n",
            "  man - woman  = 0.8323\n",
            "  man - dog    = 0.5643\n",
            "  man - whale  = 0.3055\n",
            "  man - tree   = 0.4319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Write a Python code to train a sentence using GloVe embedding from scratch.Print the token,build the co-occurance matrix,initialize the vector and print the final vector after training the model for 3000 epochs.**\n"
      ],
      "metadata": {
        "id": "WkeSRP8m6upj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import numpy as np # numpy was imported later, moving it up for consistency\n",
        "sentences = [\n",
        "     \"I love machine learning\",\n",
        "     \"I love deep learning\"\n",
        "]\n",
        "# Tokenize & vocab\n",
        "sentences = [s.lower().split() for s in sentences]\n",
        "vocab = sorted(set(word for s in sentences for word in s))\n",
        "print(vocab)\n",
        "stoi = {w:i for i,w in enumerate(vocab)}\n",
        "print(stoi)\n",
        "itos = {i:w for w,i in stoi.items()}\n",
        "print(itos)\n",
        "# Build co-occurrence matrix (window=1)\n",
        "X = defaultdict(float)\n",
        "window = 1\n",
        "for sentence in sentences:\n",
        "  for i, word in enumerate(sentence):\n",
        "    for j in range(max(i-window,0), min(i+window+1, len(sentence))):\n",
        "      if i != j:\n",
        "        X[(stoi[word], stoi[sentence[j]])] += 1.0\n",
        "print(X)\n",
        "# Initialize vectors\n",
        "dim = 2\n",
        "W = np.random.randn(len(vocab), dim) * 0.1\n",
        "bias = np.zeros(len(vocab))\n",
        "lr = 0.05\n",
        "# Train\n",
        "for epoch in range(3000):\n",
        "  loss = 0\n",
        "  for (i, j), xij in X.items():\n",
        "    target = math.log(xij)\n",
        "    pred = np.dot(W[i], W[j]) + bias[i] + bias[j]\n",
        "    diff = pred - target\n",
        "    loss += diff**2\n",
        "    # Gradient update\n",
        "    grad = 2 * diff\n",
        "    W[i] -= lr * grad * W[j]\n",
        "    W[j] -= lr * grad * W[i]\n",
        "    bias[i] -= lr * grad\n",
        "    bias[j] -= lr * grad\n",
        "  if epoch % 1000 == 0:\n",
        "     print(f\"epoch {epoch} loss {loss}\")\n",
        "for w in vocab:\n",
        "    print(w, W[stoi[w]])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abYMSXD76zUc",
        "outputId": "6754c22e-5fbf-42b0-a67a-01b7174c5643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['deep', 'i', 'learning', 'love', 'machine']\n",
            "{'deep': 0, 'i': 1, 'learning': 2, 'love': 3, 'machine': 4}\n",
            "{0: 'deep', 1: 'i', 2: 'learning', 3: 'love', 4: 'machine'}\n",
            "defaultdict(<class 'float'>, {(1, 3): 2.0, (3, 1): 2.0, (3, 4): 1.0, (4, 3): 1.0, (4, 2): 1.0, (2, 4): 1.0, (3, 0): 1.0, (0, 3): 1.0, (0, 2): 1.0, (2, 0): 1.0})\n",
            "epoch 0 loss 0.7977490251701381\n",
            "epoch 1000 loss 1.1955218030417551e-12\n",
            "epoch 2000 loss 2.6525525390057237e-21\n",
            "deep [ 0.10881816 -0.09966217]\n",
            "i [-0.17165491  0.09709506]\n",
            "learning [ 0.00995003 -0.0390901 ]\n",
            "love [-0.10620272  0.16225377]\n",
            "machine [-0.05768578 -0.19571621]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Python code to load GloVe Word Vectors, each each line from the glove fi\n",
        "and create an embedding matrix for the model and create a non-trainable keras embedding layer**"
      ],
      "metadata": {
        "id": "cjqpeUzV63Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # Import Tokenizer\n",
        "# Path to GloVe file (e.g., glove.6B.100d.txt)\n",
        "glove_path = \"/content/glove.6B.100d.txt\"\n",
        "# 1. Load GloVe vectors\n",
        "embeddings_index = {}\n",
        "with open(glove_path, encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = vector\n",
        "print(f\"Loaded {len(embeddings_index)} word vectors from GloVe.\")\n",
        "embedding_dim = 100 # or 50/200/300 based on GloVe file used\n",
        "\n",
        "word_index = {\"the\": 1, \"a\": 2, \"of\": 3, \"to\": 4, \"in\": 5, \"and\": 6, \"king\": 7, \"prince\": 8, \"queen\": 9, \"man\":10, \"woman\":11}\n",
        "\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "#2. Create embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  vec = embeddings_index.get(word)\n",
        "  if vec is not None:\n",
        "     embedding_matrix[i] = vec\n",
        "print(\"Embedding matrix shape:\", embedding_matrix.shape)\n",
        "print(embedding_matrix)\n",
        "# 3. Define Keras Embedding layer (non-trainable)\n",
        "embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            trainable=False)\n",
        "print(\"\\nKeras Embedding layer created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdHTyYgo64Dk",
        "outputId": "c48dacd0-ff87-46f4-a494-b5cb63142574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 word vectors from GloVe.\n",
            "Embedding matrix shape: (12, 100)\n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-0.038194   -0.24487001  0.72812003 ... -0.1459      0.82779998\n",
            "   0.27061999]\n",
            " [-0.27085999  0.044006   -0.02026    ... -0.4923      0.63687003\n",
            "   0.23642001]\n",
            " ...\n",
            " [-0.50045002 -0.70826     0.55387998 ... -0.36032     0.13347\n",
            "  -0.56075001]\n",
            " [ 0.37292999  0.38503     0.71086001 ... -0.93711001  0.039138\n",
            "  -0.53911   ]\n",
            " [ 0.59368002  0.44825     0.59320003 ... -0.54648     0.15162\n",
            "  -0.30754   ]]\n",
            "\n",
            "Keras Embedding layer created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. Write python code to perform spam detection using word embedding.**"
      ],
      "metadata": {
        "id": "kZJteh3A66yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Dataset Simulation\n",
        "data = [\n",
        "    (\"URGENT! You have won a 1 week FREE membership in our prize draw. Txt 'CLAIM' to 81020.\", 1), # Spam\n",
        "    (\"Hey dad, I'm stuck in traffic, running late for dinner.\", 0), # Ham\n",
        "    (\"Congratulations! You've been selected for a ␖2000 holiday voucher. Text YES to 88000 now!\", 1), # Spam\n",
        "    (\"Call me later to discuss the project update. I need your input.\", 0), # Ham\n",
        "    (\"WINNER! Claim your FREE iPhone 15 by texting 'FREE' to 60900. Hurry up!\", 1), # Spam\n",
        "    (\"Ok thanks. See you tomorrow.\", 0) # Ham\n",
        "]\n",
        "texts_8 = [d[0] for d in data]\n",
        "labels_8 = np.array([d[1] for d in data])\n",
        "\n",
        "# Configuration\n",
        "MAX_WORDS = 1000 # Max number of words to keep in the vocabulary\n",
        "MAX_LEN = 20 # Max sequence length\n",
        "EMBEDDING_DIM_8 = 50 # Using 50 for simplicity\n",
        "GLOVE_EMBEDDING_DIM = 100 # Defining GLOVE_EMBEDDING_DIM\n",
        "# 2. Tokenization and Sequencing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer_8 = Tokenizer(num_words=MAX_WORDS)\n",
        "tokenizer_8.fit_on_texts(texts_8)\n",
        "sequences_8 = tokenizer_8.texts_to_sequences(texts_8)\n",
        "padded_sequences_8 = pad_sequences(sequences_8, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "word_index_8 = tokenizer_8.word_index\n",
        "VOCAB_SIZE_8 = len(word_index_8) + 1\n",
        "\n",
        "# 3. Embedding Matrix Creation (Using simulated GloVe, similar to Task 7)\n",
        "embedding_matrix_8 = np.zeros((VOCAB_SIZE_8, EMBEDDING_DIM_8))\n",
        "if GLOVE_EMBEDDING_DIM >= EMBEDDING_DIM_8:\n",
        "  print(f\" Creating embedding matrix for {VOCAB_SIZE_8} words, initializing from GloVe.\")\n",
        "  for word, i in word_index_8.items():\n",
        "    # Fallback if the word is not in the small dummy GLOVE_VECTORS\n",
        "    if i < VOCAB_SIZE_8 and i > 0:\n",
        "      embedding_matrix_8[i] = np.random.rand(EMBEDDING_DIM_8) # Mock vector initialization\n",
        "\n",
        "# 4. Model Definition\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense # Added Flatten\n",
        "model_8 = Sequential([\n",
        "      Embedding(\n",
        "             input_dim=VOCAB_SIZE_8,\n",
        "             output_dim=EMBEDDING_DIM_8,\n",
        "             input_length=MAX_LEN,\n",
        "             trainable=True\n",
        "      ),\n",
        "      Flatten(),\n",
        "      Dense(24, activation='relu'),\n",
        "      Dense(1, activation='sigmoid') # Binary classification\n",
        "])\n",
        "model_8.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\n Spam Detection Model Summary (Keras):\")\n",
        "model_8.summary()\n",
        "\n",
        "# 5. Training (Using the simulated tiny dataset)\n",
        "print(\"\\n Training the Spam Detection Model (Simulated):\")\n",
        "history = model_8.fit(\n",
        "    padded_sequences_8,\n",
        "    labels_8,\n",
        "    epochs=10,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# 6. Evaluation\n",
        "loss, accuracy = model_8.evaluate(padded_sequences_8, labels_8, verbose=0)\n",
        "print(f\" Final training accuracy (on small data): {accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "AbtFGOxu6_TO",
        "outputId": "606373d0-e588-44b8-fea1-23be6a5196b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Creating embedding matrix for 63 words, initializing from GloVe.\n",
            "\n",
            " Spam Detection Model Summary (Keras):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training the Spam Detection Model (Simulated):\n",
            " Final training accuracy (on small data): 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Prediction Example\n",
        "new_spam_text = [\"Free gift card! Click here now to claim.\"]\n",
        "new_sequences = tokenizer_8.texts_to_sequences(new_spam_text)\n",
        "new_padded = pad_sequences(new_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "prediction = model_8.predict(new_padded, verbose=0)[0][0]\n",
        "label = \"SPAM\" if prediction > 0.5 else \"HAM\"\n",
        "print(f\" Test prediction for '{new_spam_text[0]}': {prediction:.4f} -> {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBA2jo-B7Bnl",
        "outputId": "396dd662-2c8b-4bff-b7f9-6496d8946fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Test prediction for 'Free gift card! Click here now to claim.': 0.3733 -> HAM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q9BmDF7h7BdE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}